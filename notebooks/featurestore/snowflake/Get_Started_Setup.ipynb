{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Snowflake Ingestion (1) Get started with the Feature Store\"\n",
    "date: 2021-02-24\n",
    "type: technical_note\n",
    "draft: false\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get started with Snowflake and the Hopsworks Feature Store\n",
    "This tutorial notebook will help you to get started working with the Hopsworks feature store and Snowflake.\n",
    "\n",
    "- We assume that you already have snowflake account. If not please follow the [tutorial](https://www.snowflake.com/webinar/virtual-hands-on-labs/virtual-hands-on-lab-2020-03-18/) to setup your Snowflake account and load telcom churn dataset   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Snowflake connector in Hopsworks <a name=\"access_snowflake\"></a>\n",
    "\n",
    "#### Step1 ) From Hopsworks go to Feature Store select Storage Connectors and click Create New\n",
    "![1.jpg](images/hopsworks/connector/1.png)\n",
    "\n",
    "    \n",
    "#### Step2 ) Fill in name and description of the connector. <br>  \n",
    "\n",
    " &nbsp; In JDBC connection string provide the connection and credentials required to connect to your Snowflake account. \n",
    "\n",
    "**NOTE:** For Snowflake accounts in regions other than US WEST add the Region ID after a period <ACCOUNT>.<REGION ID> i.e. ra96958.eu-central-1. \n",
    " \n",
    "                                                                                     \n",
    " &nbsp; then click Create\n",
    "![2.jpg](images/hopsworks/connector/2.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hsfs\n",
    "\n",
    "connection = hsfs.connection()\n",
    "\n",
    "# get a reference to the feature store, you can access also shared feature stores by providing the feature store name\n",
    "fs = connection.get_feature_store()\n",
    "connector = fs.get_storage_connector(\"snowflake_spark_connector\", \"JDBC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import snowflake.connector # tested on snowflake-connector-python==2.3.1 \n",
    "# Connecting to Snowflake using the default authenticator\n",
    "ctx = snowflake.connector.connect(\n",
    "  user='HOPSWORKS',\n",
    "  password=connector.arguments,\n",
    "  account='ra96958.eu-central-1',\n",
    "  warehouse='HOPSWORKS_WH',\n",
    "  database='ML_WORKSHOP',\n",
    "  schema='PUBLIC'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Query Snowflake Data\n",
    "cs=ctx.cursor()\n",
    "allrows=cs.execute(\"\"\"select Cust_ID,STATE,ACCOUNT_LENGTH,AREA_CODE,PHONE,INTL_PLAN,VMAIL_PLAN,VMAIL_MESSAGE,\n",
    "                   DAY_MINS,DAY_CALLS,DAY_CHARGE,EVE_MINS,EVE_CALLS,EVE_CHARGE,NIGHT_MINS,NIGHT_CALLS,\n",
    "                   NIGHT_CHARGE,INTL_MINS,INTL_CALLS,INTL_CHARGE,CUSTSERV_CALLS,\n",
    "                   CHURN from CUSTOMER_CHURN \"\"\").fetchall()\n",
    "\n",
    "churn = pd.DataFrame(allrows)\n",
    "churn.columns=['Cust_id','State','Account Length','Area Code','Phone','Intl Plan', 'VMail Plan', 'VMail Message','Day Mins',\n",
    "            'Day Calls', 'Day Charge', 'Eve Mins', 'Eve Calls', 'Eve Charge', 'Night Mins', 'Night Calls','Night Charge',\n",
    "            'Intl Mins','Intl Calls','Intl Charge','CustServ Calls', 'Churn?']\n",
    "\n",
    "pd.set_option('display.max_columns', 500)     # Make sure we can see all of the columns\n",
    "pd.set_option('display.max_rows', 10)         # Keep the output on one page\n",
    "churn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark\n",
    "#### Step 1: Attache snowflake-jdbc-3.12.8.jar to the cluster.\n",
    "#### https://mvnrepository.com/artifact/net.snowflake/snowflake-jdbc/3.12.8\n",
    "#### Step 2: Attache spark-snowflake_2.11-2.8.1 driver to the cluster.\n",
    "#### https://mvnrepository.com/artifact/net.snowflake/spark-snowflake_2.11/2.8.1-spark_2.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import com.logicalclocks.hsfs._\n",
    "import scala.collection.JavaConversions._\n",
    "import collection.JavaConverters._\n",
    "\n",
    "val connection = HopsworksConnection.builder().build();\n",
    "val fs = connection.getFeatureStore();\n",
    "val connector = fs.getStorageConnector(\"snowflake_spark_connector\",StorageConnectorType.JDBC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val SNOWFLAKE_SOURCE_NAME = \"net.snowflake.spark.snowflake\"\n",
    "val sfOptions = Map(\n",
    "\"sfURL\" -> connector.getConnectionString(),\n",
    "\"sfUser\" -> \"HOPSWORKS\",\n",
    "\"sfPassword\" -> connector.getArguments(),\n",
    "\"sfDatabase\" -> \"ML_WORKSHOP\",\n",
    "\"sfSchema\" -> \"PUBLIC\",\n",
    "\"sfWarehouse\" -> \"HOPSWORKS_WH\",\n",
    "\"sfRole\" -> \"HOPSWORKS_ROLE\"\n",
    ")\n",
    "\n",
    "val df = spark.read.\n",
    "format(SNOWFLAKE_SOURCE_NAME).\n",
    "options(sfOptions).\n",
    "option(\"dbtable\", \"CUSTOMER_CHURN\").\n",
    "load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
